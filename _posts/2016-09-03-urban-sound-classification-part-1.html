---
layout: post
title: 'Urban Sound Classification, Part 1'
subtitle: >-
  Feature extraction from sound dataset and classification using Neural
  Networks.
published: true
---
<div>
<link href="../css/prism.css" rel="stylesheet" />
<link href="../css/prism-linenumbers.css" rel="stylesheet" />
<script src="../js/prism.js"></script>
<script src="../js/prism-linenumbers.js"></script>
</div>
<div class="text-justify">
<p>We all got exposed to different sounds every day. Like, the sound of car horns, siren and music etc. How about teaching computer to classify such sounds automatically into categories! </p>
  
<p>In this blog post, we will learn techniques to classify urban sounds into categories using machine learning. Earlier blog posts covered classification problems where data can be easily expressed in vector form.  For example, in the textual dataset, each word in the corpus becomes feature and tf-idf score becomes its value. Likewise, in <a href="http://aqibsaeed.github.io/2016-08-10-logistic-regression-tf/" target="_blank">anomaly detection dataset</a><em>&nbsp;</em>we saw two features &ldquo;throughput&rdquo; and &ldquo;latency&rdquo; that fed into a classifier. But when it comes to sound, feature extraction is not quite straightforward. Today, we will first see what features can be extracted from sound data and how easy it is to extract such features in Python using open source library called <a href="http://librosa.github.io">Librosa</a>.</p>
<p>To get started with this tutorial, please make sure you have following tools installed:</p>
<ul>
<li>Tensorflow</li>
<li>Librosa</li>
<li>Numpy</li>
<li>Matplotlib</li>
</ul>
</div>
<div class="text-justify">
<h2>Dataset</h2>
<p>We need a labelled dataset that we can feed into machine learning algorithm. Fortunately, some researchers published <a href="https://serv.cusp.nyu.edu/projects/urbansounddataset/" target="_blank"> urban sound dataset.</a> It contains 8,732 labelled sound clips (4 seconds each) from ten classes: <em>air conditioner, car horn, children playing, dog bark, drilling, engine idling, gunshot, jackhammer, siren, and street music. </em>The dataset by default is divided into 10-folds. To get the dataset please visit the following <a href="https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html" target="_blank">link</a>&nbsp;and if you want to use this dataset in your research kindly don&rsquo;t forget to acknowledge. In this dataset, the sound files are in <em>.wav</em> format but if you have files in another format such as <em>.mp3</em>, then it&rsquo;s good to convert them into <em>.wav</em> format. It&rsquo;s because <em>.mp3</em> is lossy music compression technique, check this <a href="http://www.premiumbeat.com/blog/when-to-use-wav-files-when-to-use-mp3-files-what-is-the-difference-between-the-two-formats/" target="_blank">link</a>&nbsp;for more information. To keep things simple, we will use sound files from only first three folds, namely fold1, fold2 and fold3.</p>
<p>Let&rsquo;s read some sound files and visualise to understand how different each sound clip is from other. Matplotlib&rsquo;s <code>specgram</code> method performs all the required calculation and plotting of the spectrum. Likewise, Librosa provide handy method for wave and log power spectrogram plotting. By looking at the plots shown in Figure 1, 2 and 3, we can see apparent differences between sound clips of different classes.</p>

<pre><code class="language-python line-numbers">import glob
import os
import librosa
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from matplotlib.pyplot import specgram
%matplotlib inline

def load_sound_files(file_paths):
    raw_sounds = []
    for fp in file_paths:
        X,sr = librosa.load(fp)
        raw_sounds.append(X)
    return raw_sounds

def plot_waves(sound_names,raw_sounds):
    i = 1
    fig = plt.figure(figsize=(25,60), dpi = 900)
    for n,f in zip(sound_names,raw_sounds):
        plt.subplot(10,1,i)
        librosa.display.waveplot(np.array(f),sr=22050)
        plt.title(n.title())
        i += 1
    plt.suptitle('Figure 1: Waveplot',x=0.5, y=0.915,fontsize=18)
    plt.show()
    
def plot_specgram(sound_names,raw_sounds):
    i = 1
    fig = plt.figure(figsize=(25,60), dpi = 900)
    for n,f in zip(sound_names,raw_sounds):
        plt.subplot(10,1,i)
        specgram(np.array(f), Fs=22050)
        plt.title(n.title())
        i += 1
    plt.suptitle('Figure 2: Spectrogram',x=0.5, y=0.915,fontsize=18)
    plt.show()

def plot_log_power_specgram(sound_names,raw_sounds):
    i = 1
    fig = plt.figure(figsize=(25,60), dpi = 900)
    for n,f in zip(sound_names,raw_sounds):
        plt.subplot(10,1,i)
        D = librosa.logamplitude(np.abs(librosa.stft(f))**2, ref_power=np.max)
        librosa.display.specshow(D,x_axis='time' ,y_axis='log')
        plt.title(n.title())
        i += 1
    plt.suptitle('Figure 3: Log power spectrogram',x=0.5, y=0.915,fontsize=18)
    plt.show()</code></pre>
  
<pre><code class="language-python line-numbers">sound_file_paths = ["57320-0-0-7.wav","24074-1-0-3.wav","15564-2-0-1.wav","31323-3-0-1.wav",
"46669-4-0-35.wav","89948-5-0-0.wav","40722-8-0-4.wav",
"103074-7-3-2.wav","106905-8-0-0.wav","108041-9-0-4.wav"]

sound_names = ["air conditioner","car horn","children playing",
"dog bark","drilling","engine idling", "gun shot",
"jackhammer","siren","street music"]

raw_sounds = load_sound_files(sound_file_paths)

plot_waves(sound_names,raw_sounds)
plot_specgram(sound_names,raw_sounds)
plot_log_power_specgram(sound_names,raw_sounds)</code></pre>

<p style="text-align:center"><img src="../img/urban-sound-wave-plot.png" alt="Sound Wave Plot"/></p> 
  
<p style="text-align:center"><img src="../img/urban-sound-spectrogram.png" alt="Sound Spectrogram"/></p> 
 
 <p style="text-align:center"><img src="../img/urban-sound-logpower-spectrogram.png" alt="Sound Log Power Spectrogram"/></p> 
  
</div> 
  
<div class="text-justify">
<h2>Feature Extraction</h2>
<p>To extract the useful features from sound data, we will use <em>Librosa</em> library. It provides several methods to extract different features from the sound clips. We are going to use below mentioned methods to extract various features:</p>
<ul>
<li><em>melspectrogram</em>: Compute a Mel-scaled power spectrogram</li>
<li><em>mfcc</em>: Mel-frequency cepstral coefficients</li>
<li><em>chorma-stft</em>: Compute a chromagram from a waveform or power spectrogram</li>
<li><em>spectral_contrast</em>: Compute spectral contrast, using method defined in [<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1035731" target="_blank">1</a>]</li>
<li><em>tonnetz</em>: Computes the tonal centroid features (tonnetz), following the method of [<a href="http://dl.acm.org/citation.cfm?id=1178727" target="_blank">2</a>]</li>
</ul>
<p>To make the process of feature extraction from sound clips easy, two helper methods are defined. First <code>parse_audio_files</code> which takes parent directory name, subdirectories within parent directory and file extension (default is .wav) as input. It then iterates over all the files within subdirectories and call second helper function <code>extract_feature</code>. It takes file path as input, read the file by calling <em>librosa.load </em>method, extract and return features discussed above. These two methods are all that is required to convert raw sound clips into informative features (along with a class label for each sound clip) that we can directly feed into our classifier. Remember, the class label of each sound clip is in the file name. For example, if the file name is <em>108041-9-0-4.wav </em>then the class label will be 9. Doing string split by &ndash; and taking the second item of the array will give us the class label.</p>
  
<pre><code class="language-python line-numbers">def extract_feature(file_name):
    X, sample_rate = librosa.load(file_name)
    stft = np.abs(librosa.stft(X))
    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)
    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)
    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)
    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)
    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X),sr=sample_rate).T,axis=0)
    return mfccs,chroma,mel,contrast,tonnetz

def parse_audio_files(parent_dir,sub_dirs,file_ext='*.wav'):
    features, labels = np.empty((0,193)), np.empty(0)
    for label, sub_dir in enumerate(sub_dirs):
        for fn in glob.glob(os.path.join(parent_dir, sub_dir, file_ext)):
            mfccs, chroma, mel, contrast,tonnetz = extract_feature(fn)
            ext_features = np.hstack([mfccs,chroma,mel,contrast,tonnetz])
            features = np.vstack([features,ext_features])
            labels = np.append(labels, fn.split('/')[2].split('-')[1])
    return np.array(features), np.array(labels, dtype = np.int)

def one_hot_encode(labels):
    n_labels = len(labels)
    n_unique_labels = len(np.unique(labels))
    one_hot_encode = np.zeros((n_labels,n_unique_labels))
    one_hot_encode[np.arange(n_labels), labels] = 1
    return one_hot_encode</code></pre>  
  
<pre><code class="language-python line-numbers">parent_dir = 'Sound-Data'
tr_sub_dirs = ['fold1','fold2']
ts_sub_dirs = ['fold3']
tr_features, tr_labels = parse_audio_files(parent_dir,tr_sub_dirs)
ts_features, ts_labels = parse_audio_files(parent_dir,ts_sub_dirs)

tr_labels = one_hot_encode(tr_labels)
ts_labels = one_hot_encode(ts_labels)</code></pre>
  
</div>  

<div class="text-justify">
<h2>Classification using Multilayer Neural Network</h2>
<p><em>Note: If you want to use scikit-learn or any other library for training classifier, feel free to use that. The goal of this tutorial is to provide an implementation of the neural network in Tensorflow for classification tasks. </em></p>
<p>Now we have our training and testing set ready, let&rsquo;s implement two layers neural network in Tensorflow to classify each sound clip into a different category.</p>
  
<p>The code provided below defines configuration parameters required by neural network model. Such as training epochs, a number of neurones in each hidden layer and learning rate.</p>
  
<pre><code class="language-python line-numbers">training_epochs = 5000
n_dim = tr_features.shape[1]
n_classes = 10
n_hidden_units_one = 280 
n_hidden_units_two = 300
sd = 1 / np.sqrt(n_dim)
learning_rate = 0.01</code></pre>
  
<p>Now define placeholders for features and class labels, which tensor flow will fill with the data at runtime. Furthermore, define weights and biases for hidden and output layers of the network. For non-linearity, we use the sigmoid function in the first hidden layer and tanh in the second hidden layer. The output layer has softmax as non-linearity as we are dealing with multiclass classification problem. </p>
  
<pre><code class="language-python line-numbers">X = tf.placeholder(tf.float32,[None,n_dim])
Y = tf.placeholder(tf.float32,[None,n_classes])

W_1 = tf.Variable(tf.random_normal([n_dim,n_hidden_units_one], mean = 0, stddev=sd))
b_1 = tf.Variable(tf.random_normal([n_hidden_units_one], mean = 0, stddev=sd))
h_1 = tf.nn.tanh(tf.matmul(X,W_1) + b_1)


W_2 = tf.Variable(tf.random_normal([n_hidden_units_one,n_hidden_units_two], mean = 0, stddev=sd))
b_2 = tf.Variable(tf.random_normal([n_hidden_units_two], mean = 0, stddev=sd))
h_2 = tf.nn.sigmoid(tf.matmul(h_1,W_2) + b_2)


W = tf.Variable(tf.random_normal([n_hidden_units_two,n_classes], mean = 0, stddev=sd))
b = tf.Variable(tf.random_normal([n_classes], mean = 0, stddev=sd))
y_ = tf.nn.softmax(tf.matmul(h_2,W) + b)

init = tf.initialize_all_variables()</code></pre> 
  
<p>The cross-entropy cost function will be minimised using gradient descent optimizer, the code provided below initialize cost function and optimizer. Also, define and initialize variables for accuracy calculation of the prediction by model.</p>
  
<pre><code class="language-python line-numbers">cost_function = -tf.reduce_sum(Y * tf.log(y_))
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)

correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</code></pre> 
  
<p>We have all the required pieces in place. Now let&rsquo;s train neural network model, visualise whether cost is decreasing with each epoch and make prediction on the test set, using following code:</p>
  
<pre><code class="language-python line-numbers">cost_history = np.empty(shape=[1],dtype=float)
y_true, y_pred = None, None
with tf.Session() as sess:
    sess.run(init)
    for epoch in range(training_epochs):            
        _,cost = sess.run([optimizer,cost_function],feed_dict={X:tr_features,Y:tr_labels})
        cost_history = np.append(cost_history,cost)
    
    y_pred = sess.run(tf.argmax(y_,1),feed_dict={X: ts_features})
    y_true = sess.run(tf.argmax(ts_labels,1))
    print('Test accuracy: ',round(session.run(accuracy, feed_dict={X: ts_features, Y: ts_labels}) , 3))

fig = plt.figure(figsize=(10,8))
plt.plot(cost_history)
plt.axis([0,training_epochs,0,np.max(cost_history)])
plt.show()

p,r,f,s = precision_recall_fscore_support(y_true, y_pred, average='micro')
print "F-Score:", round(f,3)</code></pre>
  
 <p style="text-align:center"><img src="../img/urban-sound-nn-cost.png" alt="NN Cost Per Epoch"/></p> 
  
<p>In this tutorial, we saw how to extract features from a sound dataset and train a two layer neural network model in Tensorflow to categories sounds. I would encourage you to check the documentation of Librosa and experiment with different neural network configurations i.e. by changing number of neurons, number of hidden layers and introducing dropout etc.</p>
<p>The python notebook is available at the following <strong><span style="text-decoration: underline;"><a href="https://github.com/aqibsaeed/Urban-Sound-Classification" target="_blank">link</a></span></strong>. If you have any question or feedback please comment below.</p>
</div>
